---
id: gpt-bach-wtc
name: GPT Bach WTC (MusicGPT)
status: production
stack: [pytorch, gradio, music21, python, numpy, matplotlib]
---

# GPT Bach WTC - Generative Music AI

**ðŸŽµ HITO: Primer modelo de IA completo from scratch**

Generative Pre-trained Transformer entrenado en 33 preludios del Well-Tempered Clavier de J.S. Bach para generar mÃºsica barroca original.

## UbicaciÃ³n

`~/Desktop/gpt_bach_wtc/`

## Stack TÃ©cnico

### Core ML
- **PyTorch 2.0+** - Framework principal
- **Transformer architecture** - 4 layers, 8 heads, 4.29M params
- **Custom tokenizer** - 150 tokens estructurados

### Procesamiento Musical
- **music21** - Parsing MIDI, anÃ¡lisis armÃ³nico
- **pretty-midi** - Export/import MIDI
- **mido** - Low-level MIDI handling

### Interfaz y VisualizaciÃ³n
- **Gradio 4.0** - Web interface interactiva
- **matplotlib** - Piano roll visualization
- **numpy** - Procesamiento numÃ©rico

## Arquitectura del Modelo

```
MusicGPT (4.29M parÃ¡metros)
â”œâ”€â”€ Token Embedding (150 â†’ 256)
â”œâ”€â”€ Relative Positional Encoding
â”œâ”€â”€ Bar Positional Encoding
â”œâ”€â”€ 4Ã— Transformer Blocks
â”‚   â”œâ”€â”€ Multi-Head Attention (8 heads Ã— 32 dims)
â”‚   â”œâ”€â”€ Feed-Forward Network (256 â†’ 1024 â†’ 256)
â”‚   â””â”€â”€ Pre-norm + Residual connections
â””â”€â”€ Output Projection (weight-tied)
```

## Dataset

- **33 preludes** de Bach WTC I+II
- **Normalizados** a C major/A minor
- **160,631 tokens** totales
- **ReducciÃ³n vocabulario:** 87.5% (de ~800 a 150)

## Training

- **100 epochs** (~2 horas CPU)
- **Optimizer:** AdamW (lr=3e-4, weight_decay=0.01)
- **Scheduler:** Cosine annealing + warmup
- **Best val loss:** 1.9693 (epoch 21)
- **Gradient clipping:** max_norm=1.0

## Mejoras Implementadas

### MEJORA #1: TokenizaciÃ³n Estructurada
Orden fijo: `<NOTE> â†’ <VOICE> â†’ <DUR> â†’ <TIME>`
- Previene errores sintÃ¡cticos en generaciÃ³n
- ValidaciÃ³n estructural en tests
- Roundtrip fidelity garantizada

### MEJORA #2: Relative Positional Encoding
- Embeddings posicionales aprendidos (vs sinusoidales)
- Bar positional encoding para estructura musical
- Mejor para patrones repetitivos

### MEJORA #3: Baseline Musical Metrics
6 mÃ©tricas cuantitativas:
- Out-of-key ratio
- Interval entropy
- Rhythmic variety
- Average phrase length
- Large leap ratio
- Parallel fifths detection

### MEJORA #4: Data Augmentation Escalonada
Estrategia progresiva:
- Epochs 0-9: Sin augmentation (aprender base)
- Epochs 10-29: Light (50% prob, Â±1 semitone, Â±5% tempo)
- Epochs 30+: Heavy (80% prob, Â±2 semitones, Â±15% tempo)

### MEJORA #5: Musical Early Stopping
- **Combined score:** 30% val_loss + 70% musical_quality
- Previene overfitting a loss puro
- Asegura calidad musical en generaciones

## Resultados

### EvaluaciÃ³n Musical

| MÃ©trica | Generated | Bach (GT) | Achievement |
|---------|-----------|-----------|-------------|
| **Musical Score** | 59.55/100 | 60.12/100 | **99.0%** ðŸŒŸ |
| **Out-of-key** | 6.92% | 11.99% | Mejor que Bach |
| **Interval entropy** | 5.07 | 5.27 | 96.3% |
| **Rhythmic variety** | 0.62 | 1.39 | 44.2% |

**ConclusiÃ³n:** El modelo genera mÃºsica con calidad casi indistinguible del Bach original.

### Samples Generados

- **20 samples** totales
- 10 Ã— 1000 tokens
- 5 Ã— 2000 tokens
- 5 Ã— 3000 tokens
- Todos sintÃ¡cticamente vÃ¡lidos
- Estructura polifÃ³nica correcta

## Interfaz Gradio

Web app interactiva con:
- ðŸŽšï¸ Controles: Temperature, Top-K, Top-P, Max Tokens, Seed
- ðŸŽ¹ Piano roll visualization en tiempo real
- ðŸ“Š MÃ©tricas musicales (out-of-key, entropy, variety, overall score)
- â¬‡ï¸ Export MIDI + MusicXML (MuseScore compatible)
- ðŸ“ˆ Progress tracking durante generaciÃ³n

**URL:** http://localhost:7860

## Deployment

Preparado para HuggingFace Spaces:
- âœ… `app.py` con Gradio interface
- âœ… `requirements.txt` optimizado
- âœ… README con header YAML
- âœ… `.gitignore` para Git LFS
- âœ… Script de deployment

## Aprendizajes Clave

### Transformer Architecture
- Attention mechanism from scratch
- Causal masking para autoregresivo
- Pre-norm vs post-norm (pre-norm mejor)
- Weight tying entre embeddings y output

### Musical ML
- TokenizaciÃ³n estructurada crÃ­tica para mÃºsica
- MÃ©tricas musicales > mÃ©tricas de loss
- Data augmentation en mÃºsica diferente a visiÃ³n/NLP
- Early stopping musical vs estadÃ­stico

### PyTorch Best Practices
- Gradient clipping esencial
- Learning rate warmup + cosine decay
- AdamW > Adam para transformers
- Checkpointing cada N epochs

### Training Insights
- CPU viable para modelos pequeÃ±os (<10M params)
- 33 piezas suficiente para estilo especÃ­fico
- Overfitting detectable desde epoch 20
- Best model â‰  final model (usar early stopping)

## PrÃ³ximos Pasos

- [ ] Deploy a HuggingFace Spaces
- [ ] Expandir dataset (mÃ¡s preludios, fugas)
- [ ] Fine-tuning con early stopping mÃ¡s agresivo
- [ ] Conditioned generation (por tonalidad, tempo, mood)
- [ ] Multi-voice generation (fugas completas)

## Enlaces

- **Proyecto:** `~/Desktop/gpt_bach_wtc/`
- **Docs:** `docs/` (INDEX, QUICKSTART, GRADIO_INTERFACE, APP_TECHNICAL)
- **GitHub:** (pendiente de subir)
- **HuggingFace:** (pendiente de deployment)

---

**Impacto:** Primer modelo de IA completo from scratch. Nueva vÃ­a de desarrollo abierta (Generative AI + Music).

**Fecha:** 20 enero 2026
